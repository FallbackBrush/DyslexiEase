{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pandas.read_csv('./grammar correction/train.csv')\n",
    "df2 = pandas.read_csv('./grammar correction/test.csv')\n",
    "#df3 = pandas.read_parquet('./validation-00000-of-00001-317631c493ed2119.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitcoin is for $7,094 this morning, which Coin...</td>\n",
       "      <td>Bitcoin goes for $7,094 this morning, accordin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The effect of widespread dud targets two face ...</td>\n",
       "      <td>1. The effect of \"widespread dud\" targets two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tax on sales of stores for non residents are s...</td>\n",
       "      <td>Capital Gains tax on the sale of properties fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Much many brands and sellers still in the market.</td>\n",
       "      <td>Many brands and sellers still in the market.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is is the latest Maintenance release of S...</td>\n",
       "      <td>This is is the latest maintenance release of S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  Bitcoin is for $7,094 this morning, which Coin...   \n",
       "1  The effect of widespread dud targets two face ...   \n",
       "2  tax on sales of stores for non residents are s...   \n",
       "3  Much many brands and sellers still in the market.   \n",
       "4  this is is the latest Maintenance release of S...   \n",
       "\n",
       "                                              output  \n",
       "0  Bitcoin goes for $7,094 this morning, accordin...  \n",
       "1  1. The effect of \"widespread dud\" targets two ...  \n",
       "2  Capital Gains tax on the sale of properties fo...  \n",
       "3       Many brands and sellers still in the market.  \n",
       "4  This is is the latest maintenance release of S...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = df1[\"input\"].tolist()\n",
    "list2 = df2[\"input\"].tolist()\n",
    "#list3 = df3[\"Abstract\"].tolist()\n",
    "\n",
    "olist1 = df1[\"output\"].tolist()\n",
    "olist2 = df2[\"output\"].tolist()\n",
    "#olist3 = df3[\"Keywords\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125000 125000\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in list1:\n",
    "    inputs.append(i)\n",
    "for i in list2:\n",
    "    inputs.append(i)\n",
    "# for i in list3:\n",
    "#     inputs.append(i)\n",
    "    \n",
    "for i in olist1:\n",
    "    outputs.append(i)\n",
    "for i in olist2:\n",
    "    outputs.append(i)\n",
    "# for i in olist3:\n",
    "#     outputs.append(i)\n",
    "    \n",
    "print(len(inputs),len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Bitcoin is for $7,094 this morning, which CoinDesk says.',\n",
       " 'Bitcoin goes for $7,094 this morning, according to CoinDesk.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalset = [(inputs[i],outputs[i]) for i in range(len(inputs))]\n",
    "finalset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "# multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "# multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'fulltext'\n",
    "TGT_LANGUAGE = 'summary'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    #train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(finalset, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    #train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(finalset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    #val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(finalset[0:20], batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gokul\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.84 GiB (GPU 0; 12.00 GiB total capacity; 15.55 GiB already allocated; 0 bytes free; 15.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, NUM_EPOCHS\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     start_time \u001b[39m=\u001b[39m timer()\n\u001b[1;32m----> 6\u001b[0m     train_loss \u001b[39m=\u001b[39m train_epoch(transformer, optimizer)\n\u001b[0;32m      7\u001b[0m     end_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m      8\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate(transformer)\n",
      "Cell \u001b[1;32mIn[17], line 22\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer)\u001b[0m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     21\u001b[0m tgt_out \u001b[39m=\u001b[39m tgt[\u001b[39m1\u001b[39m:, :]\n\u001b[1;32m---> 22\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, logits\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), tgt_out\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     23\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\gokul\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\gokul\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\gokul\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.84 GiB (GPU 0; 12.00 GiB total capacity; 15.55 GiB already allocated; 0 bytes free; 15.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 70\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " qualitative data;qualitative visualization \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"a man riding on the back of a horse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gokul\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\jit\\_recursive.py:266: UserWarning: 'batch_first' was found in ScriptModule constants, but was not actually set in __init__. Consider removing it.\n",
      "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n",
      "c:\\Users\\gokul\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\jit\\_recursive.py:260: UserWarning: 'norm' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
      "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n"
     ]
    }
   ],
   "source": [
    "model_scripted = torch.jit.script(transformer) # Export to TorchScript\n",
    "model_scripted.save('keywordextract.pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Seq2SeqTransformer\n",
       "  (transformer): RecursiveScriptModule(\n",
       "    original_name=Transformer\n",
       "    (encoder): RecursiveScriptModule(\n",
       "      original_name=TransformerEncoder\n",
       "      (layers): RecursiveScriptModule(\n",
       "        original_name=ModuleList\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=TransformerEncoderLayer\n",
       "          (self_attn): RecursiveScriptModule(\n",
       "            original_name=MultiheadAttention\n",
       "            (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "          )\n",
       "          (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "          (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "          (norm1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (norm2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (dropout1): RecursiveScriptModule(original_name=Dropout)\n",
       "          (dropout2): RecursiveScriptModule(original_name=Dropout)\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=TransformerEncoderLayer\n",
       "          (self_attn): RecursiveScriptModule(\n",
       "            original_name=MultiheadAttention\n",
       "            (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "          )\n",
       "          (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "          (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "          (norm1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (norm2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (dropout1): RecursiveScriptModule(original_name=Dropout)\n",
       "          (dropout2): RecursiveScriptModule(original_name=Dropout)\n",
       "        )\n",
       "        (2): RecursiveScriptModule(\n",
       "          original_name=TransformerEncoderLayer\n",
       "          (self_attn): RecursiveScriptModule(\n",
       "            original_name=MultiheadAttention\n",
       "            (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "          )\n",
       "          (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "          (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "          (norm1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (norm2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (dropout1): RecursiveScriptModule(original_name=Dropout)\n",
       "          (dropout2): RecursiveScriptModule(original_name=Dropout)\n",
       "        )\n",
       "      )\n",
       "      (norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "    )\n",
       "    (decoder): RecursiveScriptModule(\n",
       "      original_name=TransformerDecoder\n",
       "      (layers): RecursiveScriptModule(\n",
       "        original_name=ModuleList\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=TransformerDecoderLayer\n",
       "          (self_attn): RecursiveScriptModule(\n",
       "            original_name=MultiheadAttention\n",
       "            (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "          )\n",
       "          (multihead_attn): RecursiveScriptModule(\n",
       "            original_name=MultiheadAttention\n",
       "            (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "          )\n",
       "          (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "          (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "          (norm1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (norm2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (norm3): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (dropout1): RecursiveScriptModule(original_name=Dropout)\n",
       "          (dropout2): RecursiveScriptModule(original_name=Dropout)\n",
       "          (dropout3): RecursiveScriptModule(original_name=Dropout)\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=TransformerDecoderLayer\n",
       "          (self_attn): RecursiveScriptModule(\n",
       "            original_name=MultiheadAttention\n",
       "            (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "          )\n",
       "          (multihead_attn): RecursiveScriptModule(\n",
       "            original_name=MultiheadAttention\n",
       "            (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "          )\n",
       "          (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "          (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "          (norm1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (norm2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (norm3): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (dropout1): RecursiveScriptModule(original_name=Dropout)\n",
       "          (dropout2): RecursiveScriptModule(original_name=Dropout)\n",
       "          (dropout3): RecursiveScriptModule(original_name=Dropout)\n",
       "        )\n",
       "        (2): RecursiveScriptModule(\n",
       "          original_name=TransformerDecoderLayer\n",
       "          (self_attn): RecursiveScriptModule(\n",
       "            original_name=MultiheadAttention\n",
       "            (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "          )\n",
       "          (multihead_attn): RecursiveScriptModule(\n",
       "            original_name=MultiheadAttention\n",
       "            (out_proj): RecursiveScriptModule(original_name=NonDynamicallyQuantizableLinear)\n",
       "          )\n",
       "          (linear1): RecursiveScriptModule(original_name=Linear)\n",
       "          (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          (linear2): RecursiveScriptModule(original_name=Linear)\n",
       "          (norm1): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (norm2): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (norm3): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          (dropout1): RecursiveScriptModule(original_name=Dropout)\n",
       "          (dropout2): RecursiveScriptModule(original_name=Dropout)\n",
       "          (dropout3): RecursiveScriptModule(original_name=Dropout)\n",
       "        )\n",
       "      )\n",
       "      (norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "    )\n",
       "  )\n",
       "  (generator): RecursiveScriptModule(original_name=Linear)\n",
       "  (src_tok_emb): RecursiveScriptModule(\n",
       "    original_name=TokenEmbedding\n",
       "    (embedding): RecursiveScriptModule(original_name=Embedding)\n",
       "  )\n",
       "  (tgt_tok_emb): RecursiveScriptModule(\n",
       "    original_name=TokenEmbedding\n",
       "    (embedding): RecursiveScriptModule(original_name=Embedding)\n",
       "  )\n",
       "  (positional_encoding): RecursiveScriptModule(\n",
       "    original_name=PositionalEncoding\n",
       "    (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.jit.load('keywordextract.pt')\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
